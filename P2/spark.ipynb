{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cli_spark = mySqlContext.read.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://10.126.209.159:3306/nos\",\n",
    "    driver = \"com.mysql.jdbc.Driver\",\n",
    "    dbtable = \"client_list_eurekathon\",\n",
    "    user=\"pgbia20p2g06\",\n",
    "    password=\"@mysqlHKG6QY\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "cnx = pymysql.connect(user='pgbia20p2g06', password='@mysqlHKG6QY',\n",
    "                              host='10.126.209.159',\n",
    "                              database='nos')\n",
    "\n",
    "import pandas as pd\n",
    "cli = pd.read_sql(\"SELECT * FROM client_list_eurekathon\", cnx);\n",
    "profile = pd.read_sql(\"SELECT * FROM client_profile_data_eurekathon\", cnx);\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"spark://10.126.209.173:7077\") \\\n",
    ".appName(\"grop6\") \\\n",
    ".config(\"spark.executor.instances\",\"8\") \\\n",
    ".config(\"spark.executor.cores\",\"8\") \\\n",
    ".config(\"spark.driver.memory\", \"5g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app-20210314190253-0195'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cli=spark.createDataFrame(cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sa='4329560142D21690C211BCA3EE65C83C', is_donor=nan, month_id='202005')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+\n",
      "|                  sa|is_donor|month_id|\n",
      "+--------------------+--------+--------+\n",
      "|4329560142D21690C...|     NaN|  202005|\n",
      "|E516D7AADFF328836...|     NaN|  202005|\n",
      "|1068EC724654A19CB...|     NaN|  202005|\n",
      "|92BC24B777B922C3A...|     NaN|  202005|\n",
      "|01CEC8EC107C6A174...|     NaN|  202005|\n",
      "|462FED7670A43E013...|     NaN|  202005|\n",
      "|0C8ED16E9D4C8D028...|     NaN|  202005|\n",
      "|813FA0126BC8A0CD4...|     NaN|  202005|\n",
      "|527C39ED63ECA3D59...|     NaN|  202005|\n",
      "|7064F05F2BAE1235C...|     NaN|  202005|\n",
      "|B32FCD5129C4E2165...|     NaN|  202005|\n",
      "|E390CC5F20270FA75...|     NaN|  202005|\n",
      "|C02AD9C89B6CBBBAA...|     NaN|  202005|\n",
      "|06854DE868D638E12...|     NaN|  202005|\n",
      "|C07644DC9DD13BA15...|     NaN|  202005|\n",
      "|175365940A35670E0...|     NaN|  202005|\n",
      "|6731987508A54B486...|     NaN|  202005|\n",
      "|0A5D045DB2C1D2FB4...|     NaN|  202005|\n",
      "|D7F6F031A7D320F28...|     NaN|  202005|\n",
      "|35FCB664D19930058...|     NaN|  202005|\n",
      "+--------------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cli.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = spark.createDataFrame(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli2=cli.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.56 ms, sys: 39 Âµs, total: 6.6 ms\n",
      "Wall time: 6.77 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tempo tradicional\n",
    "cli2.is_donor=cli2.is_donor.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.63 ms, sys: 1.5 ms, total: 3.13 ms\n",
      "Wall time: 219 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#tempo spark\n",
    "df_cli=df_cli.na.fill(0,subset='is_donor') #.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.454277286135693"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30.2/6.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile2 = df_profile[['sa', 'month_id', 'typeh_card_qty',\n",
    "       'typei_card_qty', 'typea_card_qty', 'typeb_card_qty', 'typec_card_qty',\n",
    "       'typed_card_qty', 'typee_card_qty', 'typef_card_qty', 'typeg_card_qty',\n",
    "       'premium_qty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df_profile2 = df_profile2.withColumn(\"month_id\", df_profile2[\"month_id\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cli = df_cli.withColumn(\"month_id_1\", df_cli[\"month_id\"].cast(IntegerType())-1)\n",
    "#client['month_id_1'] = client['month_id'].astype(int) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+----------+\n",
      "|                  sa|is_donor|month_id|month_id_1|\n",
      "+--------------------+--------+--------+----------+\n",
      "|4329560142D21690C...|     0.0|  202005|    202004|\n",
      "|E516D7AADFF328836...|     0.0|  202005|    202004|\n",
      "|1068EC724654A19CB...|     0.0|  202005|    202004|\n",
      "|92BC24B777B922C3A...|     0.0|  202005|    202004|\n",
      "|01CEC8EC107C6A174...|     0.0|  202005|    202004|\n",
      "|462FED7670A43E013...|     0.0|  202005|    202004|\n",
      "|0C8ED16E9D4C8D028...|     0.0|  202005|    202004|\n",
      "|813FA0126BC8A0CD4...|     0.0|  202005|    202004|\n",
      "|527C39ED63ECA3D59...|     0.0|  202005|    202004|\n",
      "|7064F05F2BAE1235C...|     0.0|  202005|    202004|\n",
      "|B32FCD5129C4E2165...|     0.0|  202005|    202004|\n",
      "|E390CC5F20270FA75...|     0.0|  202005|    202004|\n",
      "|C02AD9C89B6CBBBAA...|     0.0|  202005|    202004|\n",
      "|06854DE868D638E12...|     0.0|  202005|    202004|\n",
      "|C07644DC9DD13BA15...|     0.0|  202005|    202004|\n",
      "|175365940A35670E0...|     0.0|  202005|    202004|\n",
      "|6731987508A54B486...|     0.0|  202005|    202004|\n",
      "|0A5D045DB2C1D2FB4...|     0.0|  202005|    202004|\n",
      "|D7F6F031A7D320F28...|     0.0|  202005|    202004|\n",
      "|35FCB664D19930058...|     0.0|  202005|    202004|\n",
      "+--------------------+--------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cli.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row, functions as F\n",
    "\n",
    "df_profile3 = df_profile2.withColumn(\"rn\", F.row_number().over(Window.partitionBy([\"sa\",\"month_id\"]).orderBy(F.col(\"sa\").desc())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile3 = df_profile3.filter(F.col(\"rn\") == 1).drop(\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919185"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_profile3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085546"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_profile2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cli2=df_cli.drop('month_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=df_cli2.join(df_profile3,(df_cli2.sa==df_profile3.sa) & (df_cli2.month_id_1==df_profile3.month_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455379"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sa='000FD136826DC2E15756F4A94B6D7BD9', is_donor=0.0, month_id_1=202004, sa='000FD136826DC2E15756F4A94B6D7BD9', month_id=202004, typeh_card_qty=0.0, typei_card_qty=2.0, typea_card_qty=0.0, typeb_card_qty=0.0, typec_card_qty=2.0, typed_card_qty=0.0, typee_card_qty=0.0, typef_card_qty=0.0, typeg_card_qty=0.0, premium_qty=1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=dff.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = dff[dff.month_id_1 != 202007]\n",
    "teste = dff[dff.month_id_1 == 202007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sa='000FD136826DC2E15756F4A94B6D7BD9', is_donor=0.0, month_id_1=202004, sa='000FD136826DC2E15756F4A94B6D7BD9', month_id=202004, typeh_card_qty=0.0, typei_card_qty=2.0, typea_card_qty=0.0, typeb_card_qty=0.0, typec_card_qty=2.0, typed_card_qty=0.0, typee_card_qty=0.0, typef_card_qty=0.0, typeg_card_qty=0.0, premium_qty=1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "treino=reduce(DataFrame.drop, ['sa', 'month_id', 'month_id_1'], treino)\n",
    "teste=reduce(DataFrame.drop, ['sa', 'month_id', 'month_id_1'], teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(is_donor=0.0, typeh_card_qty=0.0, typei_card_qty=2.0, typea_card_qty=0.0, typeb_card_qty=0.0, typec_card_qty=2.0, typed_card_qty=0.0, typee_card_qty=0.0, typef_card_qty=0.0, typeg_card_qty=0.0, premium_qty=1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino=treino.drop('typei_card_qty')\n",
    "teste=teste.drop('typei_card_qty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"typeh_card_qty\", \"typea_card_qty\", \"typeb_card_qty\",\n",
    "                                      \"typec_card_qty\", \"typed_card_qty\", \"typee_card_qty\",\n",
    "                                      \"typef_card_qty\", \"typeg_card_qty\", \"premium_qty\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+--------------------+\n",
      "|is_donor|typeh_card_qty|typea_card_qty|typeb_card_qty|typec_card_qty|typed_card_qty|typee_card_qty|typef_card_qty|typeg_card_qty|premium_qty|            features|\n",
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+--------------------+\n",
      "|     0.0|           0.0|           0.0|           0.0|           2.0|           0.0|           0.0|           0.0|           0.0|        1.0| (9,[3,8],[2.0,1.0])|\n",
      "|     0.0|           1.0|           0.0|           0.0|           3.0|           0.0|           0.0|           0.0|           0.0|        2.0|(9,[0,3,8],[1.0,3...|\n",
      "|     0.0|           0.0|           0.0|           0.0|           2.0|           0.0|           0.0|           0.0|           0.0|        2.0| (9,[3,8],[2.0,2.0])|\n",
      "|     0.0|           2.0|           1.0|           1.0|           6.0|           0.0|           0.0|           0.0|           0.0|        1.0|[2.0,1.0,1.0,6.0,...|\n",
      "|     0.0|           0.0|           0.0|           0.0|           2.0|           0.0|           0.0|           0.0|           0.0|        1.0| (9,[3,8],[2.0,1.0])|\n",
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler.transform(treino).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "tmpdf = assembler.transform(treino)\n",
    "#tmpdf.select(\"features\").show(truncate=False)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdf2 = assembler.transform(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=scaler.fit(tmpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino2=scale.transform(tmpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste2=scale.transform(tmpdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+\n",
      "|is_donor|typeh_card_qty|typea_card_qty|typeb_card_qty|typec_card_qty|typed_card_qty|typee_card_qty|typef_card_qty|typeg_card_qty|premium_qty|\n",
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+\n",
      "|     0.0|           0.0|           0.0|           0.0|           2.0|           0.0|           0.0|           0.0|           0.0|        0.0|\n",
      "|     0.0|           0.0|           0.0|           1.0|           2.0|           0.0|           0.0|           0.0|           0.0|        1.0|\n",
      "|     0.0|           1.0|           0.0|           0.0|           4.0|           0.0|           0.0|           0.0|           0.0|        1.0|\n",
      "|     0.0|           0.0|           0.0|           0.0|           2.0|           0.0|           0.0|           0.0|           0.0|        0.0|\n",
      "|     0.0|           0.0|           0.0|           0.0|           1.0|           0.0|           0.0|           0.0|           0.0|        0.0|\n",
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teste.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|              scaled|\n",
      "+--------------------+--------------------+\n",
      "| (9,[3,8],[2.0,1.0])|[-0.3931936556274...|\n",
      "|(9,[0,3,8],[1.0,3...|[2.06676327259975...|\n",
      "| (9,[3,8],[2.0,2.0])|[-0.3931936556274...|\n",
      "|[2.0,1.0,1.0,6.0,...|[4.52672020082691...|\n",
      "| (9,[3,8],[2.0,1.0])|[-0.3931936556274...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treino2.select(\"features\",\"scaled\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of zeros are 345261\n"
     ]
    }
   ],
   "source": [
    "dataset_size=float(treino2.select(\"is_donor\").count())\n",
    "numNegatives=treino2.select(\"is_donor\").where('is_donor == 0').count()\n",
    "print('The number of zeros are {}'.format(numNegatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancingRatio = 0.9982940630502731\n"
     ]
    }
   ],
   "source": [
    "BalancingRatio= numNegatives/dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "treino2=treino2.withColumn(\"classWeights\", when(treino2.is_donor == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "#train.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"is_donor\", featuresCol=\"scaled\", weightCol=\"classWeights\",maxIter=10)\n",
    "model=lr.fit(treino2)\n",
    "predict_train=model.transform(treino2)\n",
    "predict_test=model.transform(teste2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|is_donor|typeh_card_qty|typea_card_qty|typeb_card_qty|typec_card_qty|typed_card_qty|typee_card_qty|typef_card_qty|typeg_card_qty|premium_qty|            features|              scaled|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     0.0|           0.0|           0.0|           0.0|           2.0|           0.0|           0.0|           0.0|           0.0|        0.0|       (9,[3],[2.0])|[-0.3931936556274...|[0.14872075379442...|[0.53711181082299...|       0.0|\n",
      "|     0.0|           0.0|           0.0|           1.0|           2.0|           0.0|           0.0|           0.0|           0.0|        1.0|(9,[2,3,8],[1.0,2...|[-0.3931936556274...|[-0.0927622546792...|[0.47682605128299...|       1.0|\n",
      "|     0.0|           1.0|           0.0|           0.0|           4.0|           0.0|           0.0|           0.0|           0.0|        1.0|(9,[0,3,8],[1.0,4...|[2.06676327259975...|[-0.8099080926400...|[0.30791008094529...|       1.0|\n",
      "|     0.0|           0.0|           0.0|           0.0|           2.0|           0.0|           0.0|           0.0|           0.0|        0.0|       (9,[3],[2.0])|[-0.3931936556274...|[0.14872075379442...|[0.53711181082299...|       0.0|\n",
      "|     0.0|           0.0|           0.0|           0.0|           1.0|           0.0|           0.0|           0.0|           0.0|        0.0|       (9,[3],[1.0])|[-0.3931936556274...|[0.56178214921579...|[0.63686479486151...|       0.0|\n",
      "+--------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"is_donor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for test set is 0.6016392003827589\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cli=spark.createDataFrame(cli)\n",
    "df_profile = spark.createDataFrame(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for test set is 0.6016477241657463\n",
      "CPU times: user 306 ms, sys: 105 ms, total: 411 ms\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_cli=df_cli.na.fill(0,subset='is_donor')\n",
    "\n",
    "df_profile2 = df_profile[['sa', 'month_id', 'typeh_card_qty',\n",
    "       'typei_card_qty', 'typea_card_qty', 'typeb_card_qty', 'typec_card_qty',\n",
    "       'typed_card_qty', 'typee_card_qty', 'typef_card_qty', 'typeg_card_qty',\n",
    "       'premium_qty']]\n",
    "\n",
    "df_profile2 = df_profile2.withColumn(\"month_id\", df_profile2[\"month_id\"].cast(IntegerType()))\n",
    "df_cli = df_cli.withColumn(\"month_id_1\", df_cli[\"month_id\"].cast(IntegerType())-1)\n",
    "\n",
    "df_profile3 = df_profile2.withColumn(\"rn\", F.row_number().over(Window.partitionBy([\"sa\",\"month_id\"]).orderBy(F.col(\"sa\").desc())))\n",
    "df_profile3 = df_profile3.filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "df_cli2=df_cli.drop('month_id')\n",
    "dff=df_cli2.join(df_profile3,(df_cli2.sa==df_profile3.sa) & (df_cli2.month_id_1==df_profile3.month_id))\n",
    "\n",
    "dff=dff.na.fill(0)\n",
    "\n",
    "treino = dff[dff.month_id_1 != 202007]\n",
    "teste = dff[dff.month_id_1 == 202007]\n",
    "\n",
    "treino=reduce(DataFrame.drop, ['sa', 'month_id', 'month_id_1'], treino)\n",
    "teste=reduce(DataFrame.drop, ['sa', 'month_id', 'month_id_1'], teste)\n",
    "\n",
    "treino=treino.drop('typei_card_qty')\n",
    "teste=teste.drop('typei_card_qty')\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"typeh_card_qty\", \"typea_card_qty\", \"typeb_card_qty\",\n",
    "                                      \"typec_card_qty\", \"typed_card_qty\", \"typee_card_qty\",\n",
    "                                      \"typef_card_qty\", \"typeg_card_qty\", \"premium_qty\"],outputCol=\"features\")\n",
    "\n",
    "tmpdf = assembler.transform(treino)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled\"\n",
    ")\n",
    "\n",
    "tmpdf2 = assembler.transform(teste)\n",
    "\n",
    "scale=scaler.fit(tmpdf)\n",
    "treino2=scale.transform(tmpdf)\n",
    "teste2=scale.transform(tmpdf2)\n",
    "\n",
    "dataset_size=float(treino2.select(\"is_donor\").count())\n",
    "numNegatives=treino2.select(\"is_donor\").where('is_donor == 0').count()\n",
    "BalancingRatio= numNegatives/dataset_size\n",
    "\n",
    "treino2=treino2.withColumn(\"classWeights\", when(treino2.is_donor == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"is_donor\", featuresCol=\"scaled\", weightCol=\"classWeights\",maxIter=10)\n",
    "model=lr.fit(treino2)\n",
    "predict_train=model.transform(treino2)\n",
    "predict_test=model.transform(teste2)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"is_donor\")\n",
    "\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "cnx = pymysql.connect(user='pgbia20p2g06', password='@mysqlHKG6QY',\n",
    "                              host='10.126.209.159',\n",
    "                              database='nos')\n",
    "\n",
    "import pandas as pd\n",
    "client = pd.read_sql(\"SELECT * FROM client_list_eurekathon\", cnx);\n",
    "prof = pd.read_sql(\"SELECT * FROM client_profile_data_eurekathon\", cnx);\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli=client.copy()\n",
    "\n",
    "profile=prof.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for test set is 0.5834690440179761 \n",
      "\n",
      "CPU times: user 8.17 s, sys: 3.15 s, total: 11.3 s\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cli['is_donor'].fillna(0,inplace=True)\n",
    "profile = profile[['sa', 'month_id', 'typeh_card_qty',\n",
    "       'typei_card_qty', 'typea_card_qty', 'typeb_card_qty', 'typec_card_qty',\n",
    "       'typed_card_qty', 'typee_card_qty', 'typef_card_qty', 'typeg_card_qty',\n",
    "       'premium_qty']]\n",
    "\n",
    "profile.month_id = profile.month_id.astype(int)\n",
    "cli['month_id_1'] = cli['month_id'].astype(int) - 1\n",
    "profile=profile.groupby(['sa','month_id']).tail(1)\n",
    "\n",
    "cli.drop('month_id', axis = 1, inplace = True)\n",
    "merge_df = cli.merge(profile, how = 'left', left_on = ['sa', 'month_id_1'],  right_on = ['sa', 'month_id'])\n",
    "merge_df.fillna(0, inplace = True)\n",
    "\n",
    "treino = merge_df.loc[merge_df.month_id_1 != 202007]\n",
    "teste = merge_df.loc[merge_df.month_id_1 == 202007]\n",
    "\n",
    "treino.drop(['sa', 'month_id', 'month_id_1'], axis = 1, inplace = True)\n",
    "teste.drop(['sa', 'month_id', 'month_id_1'], axis = 1, inplace = True)\n",
    "\n",
    "treino.drop('typei_card_qty', axis = 1, inplace = True)\n",
    "teste.drop('typei_card_qty', axis = 1, inplace = True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(treino.iloc[:, 1:])\n",
    "X_teste = scaler.transform(teste.iloc[:, 1:])\n",
    "\n",
    "y_train = treino.iloc[:, 0]\n",
    "y_teste = teste.iloc[:, 0]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model2 = LogisticRegression(class_weight = 'balanced',random_state = 0)\n",
    "model2.fit(X_train, y_train)\n",
    "prob_results2 = model2.predict_proba(X_teste)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_teste.values, prob_results2[:,1])\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "print(\"The area under ROC for test set is\", auc_score, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
